---
title: "cwb-cads functionality"
date: "July 24, 2024"
author: Philipp Heinrich
editor: source
format:
  html:
    toc: true
    toc-float: true
    collapsed: false
    smooth-scroll: false
    number-sections: true
    df-print: paged
---

# Setup

```{r, message = FALSE}
rm(list = ls())
library(tidyverse)
library(httr2)
library(jsonlite)
library(ggrepel)
url_api <- "http://127.0.0.1:5000"
```

- get API access token via `/user/login`
```{r, message = FALSE}
access.token <- str_interp("${url_api}/user/login") |> 
  request() |> 
  req_method("POST") |> 
  req_body_form(password = 'mmda-admin', username = 'admin') |> 
  req_perform() |>
  magrittr::extract2("body") |>
  rawToChar() |> fromJSON() |>
  magrittr::extract2("access_token")
```

# Corpora

- corpus access via `/corpus/`
```{r}
corpora <- str_interp("${url_api}/corpus/") |> 
  request() |> 
  req_auth_bearer_token(access.token) |> 
  req_perform() |> 
  magrittr::extract2("body") |>
  rawToChar() |> fromJSON() |> 
  tibble()

corpora
```

- we use `"SZ-2018-2022"` as an example here
```{r}
corpus.id <- corpora |>
  filter(cwb_id == "SZ-2018-2022") |> 
  pull(id)

corpus <- str_interp("${url_api}/corpus/${corpus.id}") |> 
  request() |> 
  req_auth_bearer_token(access.token) |>
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON() 
```

## Meta Data

- indexed structural (and positional) attributes can be accessed directly at `/corpus/<id>/`
```{r}
corpus |> magrittr::extract2("p_atts")

corpus |> magrittr::extract2("s_atts")

corpus |> magrittr::extract2("s_annotations")
```

- `s_annotations` belong to `s_atts` (as long as the corpus was correctly indexed)
- in this context, we refer to the `s_att` as `level` with corresponding `key`s (such that meta data is stored in `level_key`)
- they are not stored in the database by default

```{r}
str_interp("${url_api}/corpus/${corpus.id}/meta/") |> 
  request() |> 
  req_auth_bearer_token(access.token) |>
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON() 
```

- we can store meta data from annotated s-attributes (`level_key`) using a `PUT` request
- we specify the `value_type` here (use `"unicode"` for categorical data)
- this runs idempotently
```{r}
str_interp("${url_api}/corpus/${corpus.id}/meta/") |> 
  request() |> 
  req_method('PUT') |> 
  req_body_json(list(level = 'text', key = 'year', value_type = 'unicode')) |> 
  req_auth_bearer_token(access.token) |>
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON()
```

```{r}
str_interp("${url_api}/corpus/${corpus.id}/meta/") |> 
  request() |> 
  req_auth_bearer_token(access.token) |>
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON() |> tibble() |> 
  magrittr::extract2("annotations") 
```

- and corresponding frequency counts (only reasonable for categorical data)
```{r}
str_interp("${url_api}/corpus/${corpus.id}/meta/frequencies") |> 
  request() |> 
  req_url_query(level = 'text', key = 'year') |> 
  req_auth_bearer_token(access.token) |>
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON()
```

- if you try to do this without storing the meta data first, you'll get a `404`
```{r, error = TRUE}
str_interp("${url_api}/corpus/${corpus.id}/meta/frequencies") |> 
  request() |> 
  req_url_query(level = 'text', key = 'month') |> 
  req_auth_bearer_token(access.token) |>
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON()
```

## Subcorpora

- we create subcorpora for each year in `"SZ-2018-2022"`
- we need `level`, `key`, the corresponding `values_unicode`, and the `name` of the subcorpus
```{r}
subcorpora <- str_interp("${url_api}/corpus/${corpus.id}/subcorpus/") |> 
  request() |> 
  req_auth_bearer_token(access.token) |> 
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON()

for (y in 2018:2022){
  # we only create the subcorpus if it does not already exist
  .subcorpus.exists <- F
  if (length(subcorpora) > 0){
    if (length(subcorpora |> filter(name == as.character(y), corpus.id == corpus.id)) > 0){
      .subcorpus.exists <- T
    }
  }
  if (!.subcorpus.exists){
    subcorpus <- str_interp("${url_api}/corpus/${corpus.id}/subcorpus/") |> 
      request() |> 
      req_method('PUT') |> 
      req_body_json(data = list(level = 'text', key = 'year', values_unicode = list(paste0("y", as.character(y))), name = as.character(y))) |> 
      req_auth_bearer_token(access.token) |> 
      req_perform() |> 
      magrittr::extract2("body") |> rawToChar() |> fromJSON() 
  }
}
```

```{r}
subcorpora <- str_interp("${url_api}/corpus/${corpus.id}/subcorpus/") |> 
  request() |> 
  req_auth_bearer_token(access.token) |> 
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON()

subcorpora
```

# Keyword Analysis

- keyword analyses are comparisons between two frequency lists ("target" and "reference")
- a frequency list can either be compiled from a whole corpus or a subcorpus
- two types of keyword analysis are supported:
  + (sub-)corpus 1 vs. (sub-)corpus 2
  + subcorpus vs. rest of the corpus
- mode is detected automatically
  + i.e. providing a subcorpus as the target and its original corpus as the reference will automatically switch to subcorpus vs. rest (if not suppressed)

## Example: SZ-2018-2022 vs. FAZ-2020-2022

```{r}
corpus.id <- corpora |> filter(cwb_id == "SZ-2018-2022") |> pull(id)
corpus.id.reference <- corpora |> filter(cwb_id == "FAZ-2020-2022") |> pull(id)
```

### Creation

- different positional attributes can be compared to one another (`p` and `p_reference`)
  + default: `lemma` (fallback if not defined in corpus: `word`)
```{r}
kw.id <- request(str_interp("${url_api}/keyword/")) |> 
  req_body_json(list(corpus_id = corpus.id,
                     corpus_id_reference = corpus.id.reference)) |> 
  req_auth_bearer_token(access.token) |> 
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON() |> 
  magrittr::extract2("id")
```

### Keyword Table

- the result of a keyword analysis is a frequency comparison table of individual `items` (i.e. `p`-types) with corresponding association measures (`scores`)
```{r}
kw.table.page <- request(str_interp("${url_api}/keyword/${kw.id}/items")) |>
  req_auth_bearer_token(access.token) |> 
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON()

kw.table.page |> 
  magrittr::extract2("items") |> 
  magrittr::extract2("scores") |> 
  bind_rows(.id = "item") |>
  pivot_wider(names_from = measure, values_from = score) |> 
  mutate(item = kw.table.page |> magrittr::extract2("items") |> magrittr::extract2("item"))
```

- by default, the top `page_size=10` items according to `sort_by="conservative_log_ratio"` are given
- you can switch between different association measures and paginate through results
```{r}
kw.table.page <- request(str_interp("${url_api}/keyword/${kw.id}/items")) |>
  req_url_query(sort_by = "log_likelihood", page_size = 20, page_number = 2) |> 
  req_auth_bearer_token(access.token) |> 
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON()

kw.table.page |> 
  magrittr::extract2("items") |> 
  magrittr::extract2("scores") |> 
  bind_rows(.id = "item") |>
  pivot_wider(names_from = measure, values_from = score) |> 
  mutate(item = kw.table.page |> magrittr::extract2("items") |> magrittr::extract2("item"))
```

## Example: SZ-2020 vs. SZ-2019
```{r}
subcorpus.id <- subcorpora |> filter(name == "2020") |> pull(id)
subcorpus.id.reference <- subcorpora |> filter(name == "2019") |> pull(id)
```

### Creation
```{r}
kw.id <- request(str_interp("${url_api}/keyword/")) |> 
  req_body_json(list(corpus_id = corpus.id,
                     subcorpus_id = subcorpus.id,
                     corpus_id_reference = corpus.id.reference,
                     subcorpus_id_reference = subcorpus.id.reference,
                     p = 'lemma',
                     p_reference = 'lemma')) |> 
  req_auth_bearer_token(access.token) |> 
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON() |> 
  magrittr::extract2("id")
```

### Keyword Table
```{r}
kw.items <- request(str_interp("${url_api}/keyword/${kw.id}/items")) |>
  req_auth_bearer_token(access.token) |> 
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON()
  
kw.items |> 
  magrittr::extract2("items") |> 
  magrittr::extract2("scores") |> 
  bind_rows(.id = "item") |>
  pivot_wider(names_from = measure, values_from = score) |> 
  mutate(item = kw.items |> magrittr::extract2("items") |> magrittr::extract2("item"))
```

## Example: SZ-2020 vs. remainder of SZ-2018-2022

### Creation
```{r}
kw.id <- request(str_interp("${url_api}/keyword/")) |> 
  req_body_json(list(corpus_id = corpus.id,
                     subcorpus_id = subcorpus.id,
                     corpus_id_reference = corpus.id,
                     p = 'lemma',
                     p_reference = 'lemma')) |> 
  req_auth_bearer_token(access.token) |> 
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON() |> 
  magrittr::extract2("id")
```

### Keyword Table
```{r}
kw.items <- request(str_interp("${url_api}/keyword/${kw.id}/items")) |>
  req_auth_bearer_token(access.token) |> 
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON() |> 
  magrittr::extract2("items")
  
kw.items$scores |> 
  bind_rows(.id = "item") |>
  pivot_wider(names_from = measure, values_from = score) |> 
  mutate(item = kw.items$item)
```

### Concordance Lines

- here: concordances of specific items on the keyword list
- concordances always belong to a query (in this case: a query for the item)
- we create a query using the assisted query endpoint (`/query/assisted/`), see below for more details
- note that we performed the keyword analysis on the `p="lemma"` layer

```{r}
query <- request(str_interp("${url_api}/query/assisted")) |> 
  req_body_json(list(p = "lemma", items = list("Corona-Bonds"), corpus_id = corpus.id)) |> 
  req_auth_bearer_token(access.token) |> 
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON()

query.id <- query |> magrittr::extract2("id")
```

- access to concordance lines via `/query/<id>/concordance`
```{r}
concordance.lines <- request(str_interp("${url_api}/query/${query.id}/concordance")) |> 
  req_auth_bearer_token(access.token) |> 
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON() |> 
  magrittr::extract2("lines")
```

- by default, all available structural data (as stored in CWB) are returned
```{r}
concordance.lines |> magrittr::extract2("structural")
```

- only two positional attributes (`primary="word"` and `secondary="lemma"`) are returned
- actual concordance lines (as verticalised text) can be found in `tokens`
```{r}
concordance.lines |> magrittr::extract2("tokens") |> magrittr::extract2(2)
```

- by default, you can paginate through random lines via `page_size` and `page_number`, see below for more details on filtering and sorting
- note that when working on a subcorpus, the query must be defined on this subcorpus, e.g. in `"SZ-2020"`:
```{r}
query <- request(str_interp("${url_api}/query/assisted")) |> 
  req_body_json(list(p = "lemma", items = list("Corona-Bonds"), corpus_id = corpus.id, subcorpus_id = subcorpus.id, s = "text")) |> 
  req_auth_bearer_token(access.token) |> 
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON()

query.id <- query |> magrittr::extract2("id")
```


```{r}
request(str_interp("${url_api}/query/${query.id}/concordance")) |> 
  req_auth_bearer_token(access.token) |> 
  req_url_query(page_size = 1000) |> 
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON() |> 
  magrittr::extract2("lines") |> 
  magrittr::extract2("structural") |> 
  pull(text_date) |> table()
```


### Visualisation (Semantic Map)

- keyword tables (and collocation tables, see below) are visualised via semantic maps
- semantic map = two-dimensional arrangement using coordinates + size = association measure
- association measures and (initial) coordinates are provided by the API
- semantic maps can e.g. display the top `page_size=200` items according to the `sort_by="conservative_log_ratio"` association measure
  + in an interactive frontend, the user should be able to set these parameters
  + we set `page_size=50` here, but a good frontend should make the map zoomable (best case: get more items until a user-defined cut-off)

- coordinates are provided automatically for all items
```{r}
kw.table.page <- request(str_interp("${url_api}/keyword/${kw.id}/items")) |>
  req_url_query(sort_by = "conservative_log_ratio", page_size = 50, page_number = 1) |> 
  req_auth_bearer_token(access.token) |> 
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON()

coordinates <- kw.table.page |>
  magrittr::extract2("coordinates") |> 
  tibble()

coordinates
```

- note that two types of coordinates are given:
  + initial coordinates as determined by the API
  + user coordinates that take precedence if defined via dragging on the semantic map by the user (see below)

```{r}
scores <- kw.table.page |>
  magrittr::extract2("items") |> 
  magrittr::extract2("scores") |> 
  bind_rows(.id = "item") |>
  pivot_wider(names_from = measure, values_from = score) |> 
  mutate(item = kw.table.page |> magrittr::extract2("items") |> magrittr::extract2("item"))

map <- scores |> 
  left_join(coordinates, by = "item") |> 
  mutate(x = if_else(!is.na(x_user), x_user, x),
         y = if_else(!is.na(y_user), y_user, y)) |> 
  select(item, conservative_log_ratio, x, y)

map |> 
  filter(conservative_log_ratio > 0) |>
  ggplot(aes(x = x, y = y, label = item)) + 
  geom_label_repel(aes(size = conservative_log_ratio), max.overlaps = Inf, point.size = NA, min.segment.length = Inf) +
  xlab("") + ylab("") +
  theme(legend.position = "bottom")
```

- we filter out all items with `conservative_log_ratio <= 0` here, which is probably reasonable
  - at the very least, items with `E11 >= O11` should not be displayed
  - no defaults implemented!
    + some reasonable defaults can be proposed though
    + best case scenario: user-defined

# Query

## Creation

- here: CQP query in a (sub-)corpus, i.e. we create a query using the full query end point here
- for assisted mode, see above

```{r}
query <- str_interp("${url_api}/query/") |> 
  request() |> 
  req_body_json(list(corpus_id = corpus.id, cqp_query = '[lemma="Klimawandel"%cd] | [lemma="global"] [lemma="Erwärmung"]')) |> 
  req_auth_bearer_token(access.token) |> 
  req_perform() |> magrittr::extract2("body") |> rawToChar() |> fromJSON()

query.id <- query |> magrittr::extract2("id")
```

## Concordancing

```{r}
concordance.lines <- request(str_interp("${url_api}/query/${query.id}/concordance")) |> 
  req_auth_bearer_token(access.token) |> 
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON() |> 
  magrittr::extract2("lines")
```

```{r}
concordance.lines |> 
  magrittr::extract2("tokens") |> 
  magrittr::extract2(1)
```

### Context Settings

- you can provide three parameters to define how much context is displayed (and what should be treated as `out_of_window`)
  + `window`: number of tokens left and right of the matches considered as immediate context
  + `extended_window`: maximum number of tokens totally returned
  + `context_break`: structural attribute to delimit context

```{r}
request(str_interp("${url_api}/query/${query.id}/concordance")) |> 
  req_auth_bearer_token(access.token) |> 
  req_url_query(window = 5, extended_window = 10, context_break = 's') |> 
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON() |> 
  magrittr::extract2("lines") |> 
  magrittr::extract2("tokens") |> 
  magrittr::extract2(10)
```

### Filtering

- we can filter concordance lines according to certain items in the context
- this only takes into consideration items in the immediate context (i.e. where `out_of_window=FALSE`)

```{r}
item = "Erderwärmung"

concordance.lines <- request(str_interp("${url_api}/query/${query.id}/concordance")) |> 
  req_url_query(filter_item = item, filter_item_p_att = 'lemma', extended_window = 10, context_break = 's') |> 
  req_auth_bearer_token(access.token) |> 
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON() |> 
  magrittr::extract2("lines")
```

```{r}
concordance.lines |> 
  magrittr::extract2("tokens") |> magrittr::extract(1)
```

### Sorting

- by default, the API provides concordances in randomised order
- use `sort_order="first"` to sort by corpus occurrence
```{r}
concordance.lines <- request(str_interp("${url_api}/query/${query.id}/concordance")) |> 
  req_url_query(sort_order = 'first', page_size = 20) |> 
  req_auth_bearer_token(access.token) |> 
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON() |> 
  magrittr::extract2("lines")
```

```{r}
concordance.lines |> pull(match_id)
```

- you can also use `sort_order="ascending"` or `sort_order="descending"`
- this has to be combined with `sort_by` (p-attribute) and `sort_by_offset` (relative to match..matchend)
- the following request retrieves e.g. concordance lines sorted by lemmata next to the match
```{r}
concordance.lines <- request(str_interp("${url_api}/query/${query.id}/concordance")) |> 
  req_url_query(sort_by_offset = 1, sort_by_p_att = 'lemma', sort_order = 'ascending', page_number = 10, page_size = 500) |> 
  req_auth_bearer_token(access.token) |> 
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON() |> 
  magrittr::extract2("lines")
```

```{r}
lemmata <- c()
for (i in 50:150){
  lemma <- concordance.lines |> 
    magrittr::extract2("tokens") |> magrittr::extract2(i) |> 
    filter(offset == 1) |> pull(secondary)
    lemmata <- c(lemmata, lemma)
}
lemmata
```


## Breakdown

- breakdown of the query shows which items were found (and their absolute and relative frequency (instances per million (ipm) in the corpus))
```{r}
breakdown <- request(str_interp("${url_api}/query/${query.id}/breakdown")) |> 
  req_auth_bearer_token(access.token) |>
  req_url_query(p = "lemma") |> 
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON() |> 
  magrittr::extract2("items") |> 
  tibble()
```

```{r}
breakdown|> select(item, ipm, freq, nr_tokens)
```

- note that nr_tokens (and ipm) is calculated taken the subcorpus size into account (if query is run on subcorpus)
```{r}
subcorpus.query.id <- str_interp("${url_api}/query/") |> 
  request() |> 
  req_body_json(list(corpus_id = corpus.id, subcorpus_id = subcorpus.id, cqp_query = '[lemma="Klimawandel"%cd] | [lemma="global"] [lemma="Erwärmung"]')) |> 
  req_auth_bearer_token(access.token) |> 
  req_perform() |> magrittr::extract2("body") |> rawToChar() |> fromJSON() |>
  magrittr::extract2("id")

request(str_interp("${url_api}/query/${subcorpus.query.id}/breakdown")) |> 
  req_auth_bearer_token(access.token) |>
  req_url_query(p = "lemma") |> 
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON() |> 
  magrittr::extract2("items") |> 
  tibble() |> 
  select(item, ipm, freq, nr_tokens)
```

## Meta Distribution

- meta data must be stored in database for this to work (see above)
- meta distribution via annotated (`key`) s-attribute (`level`) with p-attribute breakdown (`p`)
```{r}
query.meta <- request(str_interp("${url_api}/query/${query.id}/meta")) |> 
    req_auth_bearer_token(access.token) |>
    req_url_query(level = "text", key = "year", p = "lemma") |> 
    req_perform() |> 
    magrittr::extract2("body") |> rawToChar() |> fromJSON()

query.meta
```

- distribution across categorical meta data can be visualised using barplots
```{r}
query.meta |> mutate(year = lubridate::floor_date(as.Date(value, "y%Y"), "year")) |> 
  ggplot(aes(x = year, y = ipm, fill = item)) +
  geom_col()
```

- visualisation is only reasonable for categorical data
- other visualisations possible but not implemented

- again, if you try to do this without storing the meta data first, you'll get a `404`
```{r, error = TRUE}
query.meta <- request(str_interp("${url_api}/query/${query.id}/meta")) |> 
    req_auth_bearer_token(access.token) |>
    req_url_query(level = "text", key = "month", p = "lemma") |> 
    req_perform() |> 
    magrittr::extract2("body") |> rawToChar() |> fromJSON()
```

## Collocation

### Creation

- collocation analyses are linked to queries (words that appear in the vicinity of the query matches)

```{r}
collocation <- request(str_interp("${url_api}/query/${query.id}/collocation")) |> 
  req_auth_bearer_token(access.token) |>
  req_url_query(p = "lemma") |> 
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON()

collocation.id <- collocation |> magrittr::extract2("id")
```

### Collocation Table

- cf. keyword table
```{r}
coll.table.page <- request(str_interp("${url_api}/collocation/${collocation.id}/items")) |> 
  req_auth_bearer_token(access.token) |>
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON() 

coll.table.page |> 
  magrittr::extract2("items") |> 
  magrittr::extract2("scores") |> 
  bind_rows(.id = "item") |>
  pivot_wider(names_from = measure, values_from = score) |> 
  mutate(item = coll.table.page |> magrittr::extract2("items") |> magrittr::extract2("item"))
```

### Semantic Map

- cf. semantic map in keyword analyses
```{r}
coll.table.page <- request(str_interp("${url_api}/collocation/${collocation.id}/items")) |> 
  req_url_query(page_size = 50) |> 
  req_auth_bearer_token(access.token) |>
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON() 

scores <- coll.table.page |> 
  magrittr::extract2("items") |> 
  magrittr::extract2("scores") |> 
  bind_rows(.id = "item") |>
  pivot_wider(names_from = measure, values_from = score) |> 
  mutate(item = coll.table.page |> magrittr::extract2("items") |> magrittr::extract2("item"))

coordinates <- coll.table.page |>
  magrittr::extract2("coordinates") |> 
  tibble()

map <- scores |> 
  left_join(coordinates, by = "item") |> 
  mutate(x = if_else(!is.na(x_user), x_user, x),
         y = if_else(!is.na(y_user), y_user, y)) |> 
  select(item, conservative_log_ratio, x, y)

map |> 
  filter(conservative_log_ratio > 0) |>
  ggplot(aes(x = x, y = y, label = item)) + 
  geom_label_repel(aes(size = conservative_log_ratio), max.overlaps = Inf, point.size = NA, min.segment.length = Inf) +
  xlab("") + ylab("") +
  theme(legend.position = "bottom")
```

### Concordancing

- note that when clicking on an item here, concordance lines have to be filtered according to initial query _and_ the additional filter item
- you can use the additonal filter item as an argument at `/query/<id>/concordance`
```{r}
item = "Artensterben"

concordance.lines <- request(str_interp("${url_api}/query/${query.id}/concordance")) |> 
  req_url_query(filter_item = item, filter_item_p_att = 'lemma') |> 
  req_auth_bearer_token(access.token) |> 
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON() |> 
  magrittr::extract2("lines")

concordance.lines |> 
  magrittr::extract2("tokens") |> magrittr::extract2(1)
```


# Semantic Maps

- semantic maps are a powerful way to visualise semantic profiles (keyword or collocate tables)
- semantic profile = arrangement of items on $n$-best list in two-dimensional space
- coordinates are based on automatic dimensionality reduction (t-SNE / UMAP on high-dimensional type embeddings)
- automatically created coordinates do not always reflect the intended view on a semantic profile
- in an interactive setting, users should thus be able to drag items in order to adjust their coordinates
- (dropping items is useful for creating categories, see document about discourseme description)

## Creation
- semantic maps are created by default for all keyword and collocation analyses, i.e. at
  + `GET /query/<query_id>/collocation/`
  + `POST /keyword/`
- you can de-activate this behaviour setting `semantic_map_init=False`
- this makes sure that all top items of the analysis have coordinates on the newly created map
- if you provide a `semantic_map_id`, the API makes sure that all top items have coordinates on the provided map
- a combined semantic map based on items of several analyses can also be created using
  + `PUT /semantic-map/`
  
## Interacting with coordinates
- coordinates can be accessed via
  + `GET /semantic-map/<id>/coordinates/`
- all items have two sets of coordinates:
  - the initial coordinates that are automatically created, and
  - user-defined coordinates (which are all `None` after directly after creation)
- modifying the user coordinates works via
  + `PUT /semantic-map/<id>/coordinates/`
- "deleting" user coordinates means setting them to `None`
- we take the map of the above collocation analysis as example
```{r}
semantic_map.id <- collocation$semantic_map_id
```

- directly get the coordinates from the API
```{r}
coordinates <- request(str_interp("${url_api}/semantic-map/${semantic_map.id}/coordinates/")) |> 
  req_auth_bearer_token(access.token) |>
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON() 
```

- analysis scores are linked via items to these coordinates
```{r}
coll.table.page <- request(str_interp("${url_api}/collocation/${collocation.id}/items")) |> 
  req_url_query(page_size = 50) |> 
  req_auth_bearer_token(access.token) |>
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON() 

scores <- coll.table.page |> 
  magrittr::extract2("items") |> 
  magrittr::extract2("scores") |> 
  bind_rows(.id = "item") |>
  pivot_wider(names_from = measure, values_from = score) |> 
  mutate(item = coll.table.page |> magrittr::extract2("items") |> magrittr::extract2("item"))

map <- scores |> 
  left_join(coordinates, by = "item") |> 
  mutate(x = if_else(!is.na(x_user), x_user, x),
         y = if_else(!is.na(y_user), y_user, y)) |> 
  select(item, conservative_log_ratio, x, y)

map |> 
  filter(conservative_log_ratio > 0) |>
  ggplot(aes(x = x, y = y, label = item)) + 
  geom_label_repel(aes(size = conservative_log_ratio), max.overlaps = Inf, point.size = NA, min.segment.length = Inf) +
  xlab("") + ylab("") +
  theme(legend.position = "bottom")
```

- drag items to other positions
```{r}
coordinates <- request(str_interp("${url_api}/semantic-map/${semantic_map.id}/coordinates/")) |>
  req_method('put') |> 
  req_body_json(list(item = "Artensterben", x_user = 25, y_user = 20)) |> 
  req_auth_bearer_token(access.token) |> 
  req_perform() |> 
  magrittr::extract2("body") |> rawToChar() |> fromJSON()

map <- scores |> 
  left_join(coordinates, by = "item") |> 
  mutate(x = if_else(!is.na(x_user), x_user, x),
         y = if_else(!is.na(y_user), y_user, y)) |> 
  select(item, conservative_log_ratio, x, y)

map |> 
  filter(conservative_log_ratio > 0) |>
  ggplot(aes(x = x, y = y, label = item)) + 
  geom_label_repel(aes(size = conservative_log_ratio), max.overlaps = Inf, point.size = NA, min.segment.length = Inf) +
  xlab("") + ylab("") +
  theme(legend.position = "bottom")
```